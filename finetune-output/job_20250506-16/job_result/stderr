2025-05-13 16:03:14.170 | INFO     | llmtuner.wrapers.interface:main:40 - llamafactory args: Namespace(subcommand='train', model_name_or_path='/home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat', model_template='Qwen-7B-Chat', dataset='/home/tiancongcong/code/cogmait-fine-tune/data/alpaca_zh_demo.json', each_max_samples='100', output_dir='/home/tiancongcong/code/cogmait-fine-tune/finetune-output', finetuning_type='lora', val_ratio=0.1, per_device_train_batch_size=4, learning_rate=5e-05, num_train_epochs=3, max_seq_len=8192, cpu_load=False)
2025-05-13 16:03:14.170 | INFO     | llmtuner.wrapers.trainval:trval_main:123 - export_params_cmd: 
--model_name_or_path /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat --template qwen --finetuning_type lora --export_dir /home/tiancongcong/code/cogmait-fine-tune/finetune-output 
2025-05-13 16:03:14.170 | INFO     | llmtuner.wrapers.trainval:trval_main:180 - train_cmd:python /home/tiancongcong/code/cogmait-fine-tune/src/llmtuner/wrapers/../../llamafactory/train/tuner.py \
--stage sft --do_train True --finetuning_type lora --model_name_or_path /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat --template qwen --dataset alpaca_zh_demo.json --dataset_dir /home/tiancongcong/code/cogmait-fine-tune/data --val_size 0.1 --output_dir /home/tiancongcong/code/cogmait-fine-tune/finetune-output --overwrite_output_dir True --cutoff_len 8192 --learning_rate 5e-05 --per_device_train_batch_size 4 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --num_train_epochs 3 --logging_steps 10 --save_strategy epoch --evaluation_strategy epoch --metric_for_best_model eval_loss --load_best_model_at_end True --save_total_limit 1 --fp16 True --plot_loss True --max_samples 100 --lora_target c_attn
/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[INFO|tokenization_utils_base.py:2204] 2025-05-13 16:03:18,276 >> loading file qwen.tiktoken
[INFO|tokenization_utils_base.py:2204] 2025-05-13 16:03:18,276 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 16:03:18,276 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 16:03:18,276 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 16:03:18,276 >> loading file tokenizer.json
[INFO|configuration_utils.py:673] 2025-05-13 16:03:18,487 >> loading configuration file /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat/config.json
[INFO|configuration_utils.py:673] 2025-05-13 16:03:18,489 >> loading configuration file /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat/config.json
[INFO|configuration_utils.py:742] 2025-05-13 16:03:18,489 >> Model config QWenConfig {
  "_name_or_path": "/home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat",
  "architectures": [
    "QWenLMHeadModel"
  ],
  "attn_dropout_prob": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "modeling_qwen.QWenLMHeadModel"
  },
  "bf16": false,
  "emb_dropout_prob": 0.0,
  "fp16": false,
  "fp32": false,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 22016,
  "kv_channels": 128,
  "layer_norm_epsilon": 1e-06,
  "max_position_embeddings": 8192,
  "model_type": "qwen",
  "no_bias": true,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "onnx_safe": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 1.0,
  "scale_attn_weights": true,
  "seq_length": 8192,
  "softmax_in_fp32": false,
  "tie_word_embeddings": false,
  "tokenizer_class": "QWenTokenizer",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_cache_kernel": false,
  "use_cache_quantization": false,
  "use_dynamic_ntk": true,
  "use_flash_attn": "auto",
  "use_logn_attn": true,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2204] 2025-05-13 16:03:18,490 >> loading file qwen.tiktoken
[INFO|tokenization_utils_base.py:2204] 2025-05-13 16:03:18,490 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 16:03:18,490 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 16:03:18,490 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 16:03:18,490 >> loading file tokenizer.json
[INFO|configuration_utils.py:673] 2025-05-13 16:03:20,582 >> loading configuration file /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat/config.json
[INFO|configuration_utils.py:673] 2025-05-13 16:03:20,584 >> loading configuration file /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat/config.json
[INFO|configuration_utils.py:742] 2025-05-13 16:03:20,584 >> Model config QWenConfig {
  "_name_or_path": "/home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat",
  "architectures": [
    "QWenLMHeadModel"
  ],
  "attn_dropout_prob": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "modeling_qwen.QWenLMHeadModel"
  },
  "bf16": false,
  "emb_dropout_prob": 0.0,
  "fp16": false,
  "fp32": false,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 22016,
  "kv_channels": 128,
  "layer_norm_epsilon": 1e-06,
  "max_position_embeddings": 8192,
  "model_type": "qwen",
  "no_bias": true,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "onnx_safe": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 1.0,
  "scale_attn_weights": true,
  "seq_length": 8192,
  "softmax_in_fp32": false,
  "tie_word_embeddings": false,
  "tokenizer_class": "QWenTokenizer",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_cache_kernel": false,
  "use_cache_quantization": false,
  "use_dynamic_ntk": true,
  "use_flash_attn": "auto",
  "use_logn_attn": true,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:3729] 2025-05-13 16:03:20,616 >> loading weights file /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2025-05-13 16:03:20,617 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.
[INFO|configuration_utils.py:1099] 2025-05-13 16:03:20,618 >> Generate config GenerationConfig {
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:08,  1.22s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:02<00:07,  1.17s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:03<00:05,  1.13s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:04<00:04,  1.10s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:05<00:03,  1.08s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:06<00:02,  1.10s/it]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:07<00:01,  1.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.12it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.03s/it]
[INFO|modeling_utils.py:4574] 2025-05-13 16:03:29,094 >> All model checkpoint weights were used when initializing QWenLMHeadModel.

[INFO|modeling_utils.py:4582] 2025-05-13 16:03:29,094 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat.
If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.
[INFO|configuration_utils.py:1052] 2025-05-13 16:03:29,095 >> loading configuration file /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat/generation_config.json
[INFO|configuration_utils.py:1099] 2025-05-13 16:03:29,096 >> Generate config GenerationConfig {
  "chat_format": "chatml",
  "do_sample": true,
  "eos_token_id": 151643,
  "max_new_tokens": 512,
  "max_window_size": 6144,
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "top_k": 0,
  "top_p": 0.8
}

[INFO|trainer.py:667] 2025-05-13 16:03:30,641 >> Using auto half precision backend
[INFO|trainer.py:2243] 2025-05-13 16:03:31,343 >> ***** Running training *****
[INFO|trainer.py:2244] 2025-05-13 16:03:31,343 >>   Num examples = 90
[INFO|trainer.py:2245] 2025-05-13 16:03:31,343 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2025-05-13 16:03:31,343 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2249] 2025-05-13 16:03:31,343 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2025-05-13 16:03:31,343 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2251] 2025-05-13 16:03:31,343 >>   Total optimization steps = 15
[INFO|trainer.py:2252] 2025-05-13 16:03:31,344 >>   Number of trainable parameters = 4,194,304
  0%|          | 0/15 [00:00<?, ?it/s]/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
Traceback (most recent call last):
  File "/home/tiancongcong/code/cogmait-fine-tune/src/llmtuner/wrapers/../../llamafactory/train/tuner.py", line 230, in <module>
    run_exp()
  File "/home/tiancongcong/code/cogmait-fine-tune/src/llmtuner/wrapers/../../llamafactory/train/tuner.py", line 128, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/tiancongcong/code/cogmait-fine-tune/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/transformers/trainer.py", line 3485, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/code/cogmait-fine-tune/src/llamafactory/train/sft/trainer.py", line 103, in compute_loss
    return super().compute_loss(model, inputs, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/transformers/trainer.py", line 3532, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/accelerate/utils/operations.py", line 814, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/accelerate/utils/operations.py", line 802, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/peft/peft_model.py", line 1756, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/.cache/huggingface/modules/transformers_modules/qwen-7B-chat/modeling_qwen.py", line 1045, in forward
    transformer_outputs = self.transformer(
                          ^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/.cache/huggingface/modules/transformers_modules/qwen-7B-chat/modeling_qwen.py", line 882, in forward
    outputs = torch.utils.checkpoint.checkpoint(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/_compile.py", line 51, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 488, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 263, in forward
    outputs = run_function(*args)
              ^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/.cache/huggingface/modules/transformers_modules/qwen-7B-chat/modeling_qwen.py", line 878, in custom_forward
    return module(*inputs, use_cache, output_attentions)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/.cache/huggingface/modules/transformers_modules/qwen-7B-chat/modeling_qwen.py", line 612, in forward
    attn_outputs = self.attn(
                   ^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/.cache/huggingface/modules/transformers_modules/qwen-7B-chat/modeling_qwen.py", line 416, in forward
    mixed_x_layer = self.c_attn(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/peft/tuners/lora/layer.py", line 727, in forward
    result = result + lora_B(lora_A(dropout(x))) * scaling
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 44.31 GiB of which 27.00 MiB is free. Process 1520495 has 29.28 GiB memory in use. Including non-PyTorch memory, this process has 14.99 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 7.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/15 [00:00<?, ?it/s]
2025-05-13 16:03:33.075 | INFO     | llmtuner.wrapers.trainval:trval_main:183 - exit_code:
