2025-05-13 15:59:14.551 | INFO     | llmtuner.wrapers.interface:main:40 - llamafactory args: Namespace(subcommand='train', model_name_or_path='/home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat', model_template='Qwen-7B-Chat', dataset='/home/tiancongcong/code/cogmait-fine-tune/data/alpaca_zh_demo.json', each_max_samples='100', output_dir='/home/tiancongcong/code/cogmait-fine-tune/finetune-output', finetuning_type='lora', val_ratio=0.1, per_device_train_batch_size=4, learning_rate=5e-05, num_train_epochs=3, max_seq_len=8192, cpu_load=False)
2025-05-13 15:59:14.552 | INFO     | llmtuner.wrapers.trainval:trval_main:123 - export_params_cmd: 
--model_name_or_path /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat --template qwen --finetuning_type lora --export_dir /home/tiancongcong/code/cogmait-fine-tune/finetune-output 
2025-05-13 15:59:14.552 | INFO     | llmtuner.wrapers.trainval:trval_main:180 - train_cmd:python /home/tiancongcong/code/cogmait-fine-tune/src/llmtuner/wrapers/../../llamafactory/train/tuner.py \
--stage sft --do_train True --finetuning_type lora --model_name_or_path /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat --template qwen --dataset alpaca_zh_demo.json --dataset_dir /home/tiancongcong/code/cogmait-fine-tune/data --val_size 0.1 --output_dir /home/tiancongcong/code/cogmait-fine-tune/finetune-output --overwrite_output_dir True --cutoff_len 8192 --learning_rate 5e-05 --per_device_train_batch_size 4 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --num_train_epochs 3 --logging_steps 10 --save_strategy epoch --evaluation_strategy epoch --metric_for_best_model eval_loss --load_best_model_at_end True --save_total_limit 1 --fp16 True --plot_loss True --max_samples 100 --lora_target c_attn
/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[INFO|tokenization_utils_base.py:2204] 2025-05-13 15:59:18,843 >> loading file qwen.tiktoken
[INFO|tokenization_utils_base.py:2204] 2025-05-13 15:59:18,843 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 15:59:18,843 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 15:59:18,843 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 15:59:18,843 >> loading file tokenizer.json
[INFO|configuration_utils.py:673] 2025-05-13 15:59:19,042 >> loading configuration file /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat/config.json
[INFO|configuration_utils.py:673] 2025-05-13 15:59:19,043 >> loading configuration file /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat/config.json
[INFO|configuration_utils.py:742] 2025-05-13 15:59:19,044 >> Model config QWenConfig {
  "_name_or_path": "/home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat",
  "architectures": [
    "QWenLMHeadModel"
  ],
  "attn_dropout_prob": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "modeling_qwen.QWenLMHeadModel"
  },
  "bf16": false,
  "emb_dropout_prob": 0.0,
  "fp16": false,
  "fp32": false,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 22016,
  "kv_channels": 128,
  "layer_norm_epsilon": 1e-06,
  "max_position_embeddings": 8192,
  "model_type": "qwen",
  "no_bias": true,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "onnx_safe": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 1.0,
  "scale_attn_weights": true,
  "seq_length": 8192,
  "softmax_in_fp32": false,
  "tie_word_embeddings": false,
  "tokenizer_class": "QWenTokenizer",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_cache_kernel": false,
  "use_cache_quantization": false,
  "use_dynamic_ntk": true,
  "use_flash_attn": "auto",
  "use_logn_attn": true,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2204] 2025-05-13 15:59:19,045 >> loading file qwen.tiktoken
[INFO|tokenization_utils_base.py:2204] 2025-05-13 15:59:19,045 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 15:59:19,045 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 15:59:19,045 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2025-05-13 15:59:19,045 >> loading file tokenizer.json
[INFO|configuration_utils.py:673] 2025-05-13 15:59:21,094 >> loading configuration file /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat/config.json
[INFO|configuration_utils.py:673] 2025-05-13 15:59:21,095 >> loading configuration file /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat/config.json
[INFO|configuration_utils.py:742] 2025-05-13 15:59:21,096 >> Model config QWenConfig {
  "_name_or_path": "/home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat",
  "architectures": [
    "QWenLMHeadModel"
  ],
  "attn_dropout_prob": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "modeling_qwen.QWenLMHeadModel"
  },
  "bf16": false,
  "emb_dropout_prob": 0.0,
  "fp16": false,
  "fp32": false,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 22016,
  "kv_channels": 128,
  "layer_norm_epsilon": 1e-06,
  "max_position_embeddings": 8192,
  "model_type": "qwen",
  "no_bias": true,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "onnx_safe": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 1.0,
  "scale_attn_weights": true,
  "seq_length": 8192,
  "softmax_in_fp32": false,
  "tie_word_embeddings": false,
  "tokenizer_class": "QWenTokenizer",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_cache_kernel": false,
  "use_cache_quantization": false,
  "use_dynamic_ntk": true,
  "use_flash_attn": "auto",
  "use_logn_attn": true,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:3729] 2025-05-13 15:59:21,131 >> loading weights file /home/tiancongcong/.cache/modelscope/hub/models/Qwen/qwen-7B-chat/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2025-05-13 15:59:21,131 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.
[INFO|configuration_utils.py:1099] 2025-05-13 15:59:21,132 >> Generate config GenerationConfig {
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:09,  1.29s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:02<00:07,  1.20s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:03<00:05,  1.12s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:04<00:04,  1.10s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:05<00:03,  1.11s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:06<00:02,  1.12s/it]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:07<00:01,  1.13s/it]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:08<00:01,  1.15s/it]
Traceback (most recent call last):
  File "/home/tiancongcong/code/cogmait-fine-tune/src/llmtuner/wrapers/../../llamafactory/train/tuner.py", line 230, in <module>
    run_exp()
  File "/home/tiancongcong/code/cogmait-fine-tune/src/llmtuner/wrapers/../../llamafactory/train/tuner.py", line 128, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/tiancongcong/code/cogmait-fine-tune/src/llamafactory/train/sft/workflow.py", line 52, in run_sft
    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/code/cogmait-fine-tune/src/llamafactory/model/loader.py", line 172, in load_model
    model = load_class.from_pretrained(**init_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 559, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4014, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4502, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/transformers/modeling_utils.py", line 973, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/tiancongcong/miniconda3/envs/cogmait/lib/python3.12/site-packages/accelerate/utils/modeling.py", line 337, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.31 GiB of which 1014.38 MiB is free. Process 1233063 has 1012.00 MiB memory in use. Process 1520494 has 25.87 GiB memory in use. Process 1709687 has 2.88 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 768.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-13 15:59:30.681 | INFO     | llmtuner.wrapers.trainval:trval_main:183 - exit_code:
